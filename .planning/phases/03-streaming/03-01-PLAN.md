---
phase: 03-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/lib/aiAgents.ts, src/hooks/useStreamingText.ts]
autonomous: true

must_haves:
  truths:
    - "AI-generated debate content streams in real-time as tokens arrive"
    - "Text accumulates without excessive re-renders"
    - "Stream connections clean up properly on component unmount"
  artifacts:
    - path: "src/lib/aiAgents.ts"
      provides: "Streaming content generation"
      exports: ["streamSpeakerContent"]
      min_lines: 50
    - path: "src/hooks/useStreamingText.ts"
      provides: "Buffered streaming hook"
      exports: ["useStreamingText"]
      min_lines: 80
  key_links:
    - from: "src/lib/aiAgents.ts"
      to: "openai provider streaming"
      via: "streamText call with DeepSeek config"
      pattern: "streamText.*openai"
    - from: "src/hooks/useStreamingText.ts"
      to: "React state updates"
      via: "ref accumulation + rAF throttling"
      pattern: "requestAnimationFrame.*setState"
---

<objective>
Enable real-time streaming of AI-generated debate content using the Vercel AI SDK.

Purpose: Transform static AI responses into dynamic streaming content for natural debate flow
Output: Streaming-capable AI agent and buffered display hook
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-streaming/03-RESEARCH.md

# Prior work
@.planning/phases/01-core-mechanics/01-03-SUMMARY.md

# Files to modify
@src/lib/aiAgents.ts
@src/store/debateStore.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add streaming method to AI agents</name>
  <files>src/lib/aiAgents.ts</files>
  <action>
Create new `streamSpeakerContent` function alongside existing `generateSpeakerContent`:

1. Import `streamText` from 'ai' (not `generateText` - both should exist)
2. Use same function signature as `generateSpeakerContent` but return `AsyncIterable<string>` for streaming chunks
3. Configure `streamText` with:
   - Same `openai('gpt-4-turbo')` model (Note: Will use DeepSeek in production but keeping OpenAI config for now per research)
   - Same prompt construction as non-streaming version
   - Same temperature (0.7)
4. Use async generator pattern to yield chunks from stream:
   ```typescript
   const result = await streamText({ ... });
   for await (const chunk of result.textStream) {
     yield chunk;
   }
   ```
5. Include comprehensive error handling with try/catch
6. Add JSDoc explaining streaming vs non-streaming usage

Why async generator: Allows consumers to iterate stream with `for await` loops, matches Vercel AI SDK streaming pattern.

Do NOT remove existing `generateSpeakerContent` - both streaming and non-streaming will coexist initially.
  </action>
  <verify>
1. Check TypeScript compiles: `npm run build`
2. Verify import exists: `grep "import.*streamText" src/lib/aiAgents.ts`
3. Verify export exists: `grep "export.*streamSpeakerContent" src/lib/aiAgents.ts`
4. Check async generator syntax: `grep "async function\*\|AsyncIterable" src/lib/aiAgents.ts`
  </verify>
  <done>
- `streamSpeakerContent` function exists in src/lib/aiAgents.ts
- Function returns AsyncIterable<string>
- Uses streamText from Vercel AI SDK
- Comprehensive error handling in place
- TypeScript compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create buffered streaming hook</name>
  <files>src/hooks/useStreamingText.ts</files>
  <action>
Create `useStreamingText` custom hook following Pattern 1 from RESEARCH.md (Buffered State Updates):

1. Accept params: `streamGenerator: AsyncIterable<string> | null, enabled: boolean`
2. Use ref for high-frequency accumulation: `const accumulatorRef = useRef<string>('')`
3. Use state for throttled display: `const [displayText, setDisplayText] = useState<string>('')`
4. Use `requestAnimationFrame` to sync ref to state at 60fps max
5. Consume the async iterable in useEffect:
   ```typescript
   for await (const chunk of streamGenerator) {
     accumulatorRef.current += chunk;
   }
   ```
6. Return cleanup function that cancels rAF and resets state
7. Return `{ displayText, isStreaming, error }` object

Critical implementation details per RESEARCH.md:
- Use functional state updates: `setDisplayText(prev => ...)` to avoid stale closures
- Track streaming state separately from accumulator
- Handle enabled flag to allow conditional streaming
- Cancel rAF on unmount to prevent memory leaks

Why refs + rAF: Avoids 20-60 re-renders per second from token stream, maintains smooth 60fps display updates instead.
  </action>
  <verify>
1. Check TypeScript compiles: `npm run build`
2. Verify hook exports: `grep "export.*useStreamingText" src/hooks/useStreamingText.ts`
3. Check ref usage: `grep "useRef.*accumulator" src/hooks/useStreamingText.ts`
4. Check rAF cleanup: `grep "cancelAnimationFrame" src/hooks/useStreamingText.ts`
5. Verify functional updates: `grep "setDisplayText.*prev =>" src/hooks/useStreamingText.ts`
  </verify>
  <done>
- `useStreamingText` hook exists in src/hooks/useStreamingText.ts
- Uses ref-based accumulation with rAF throttling
- Proper cleanup on unmount (cancels rAF, resets state)
- Returns { displayText, isStreaming, error }
- TypeScript compiles without errors
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. **TypeScript compilation:** `npm run build` succeeds
2. **Streaming function exists:** `streamSpeakerContent` exported from aiAgents.ts
3. **Hook exists:** `useStreamingText` exported from useStreamingText.ts
4. **No console errors:** Development server runs without errors (`npm run dev`)
</verification>

<success_criteria>
- AI agents support streaming via new `streamSpeakerContent` function
- Buffered streaming hook exists with ref-based accumulation
- Proper cleanup protocols in place (rAF cancellation)
- No regression to existing non-streaming functionality
- TypeScript compilation succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/03-streaming/03-01-SUMMARY.md`
</output>
