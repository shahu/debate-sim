---
phase: 05-judge-commentary
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: 
  - src/types/judge.ts
  - src/lib/scoringSystem.ts
autonomous: true

must_haves:
  truths:
    - "System can calculate numerical scores for each speaker on a 0-100% scale"
    - "System evaluates speakers on 5 criteria: content, rebuttal, POI handling, delivery, teamwork"
    - "System generates narrative commentary explaining strengths and weaknesses"
  artifacts:
    - path: "src/types/judge.ts"
      provides: "Judge-specific type definitions for scoring and feedback"
      min_lines: 20
    - path: "src/lib/scoringSystem.ts"
      provides: "Logic for calculating scores and generating commentary"
      min_lines: 50
  key_links:
    - from: "src/lib/scoringSystem.ts"
      to: "src/types/judge.ts"
      via: "imports score types"
      pattern: "import \\{.*JudgeScore.*\\}"
---

<objective>
Create the foundational types and scoring logic for judge commentary system. This plan establishes the data structures for numerical scores and narrative feedback, along with the core logic to calculate scores based on debate transcript analysis.

Purpose: Enable post-debate evaluation with numerical scores and detailed feedback
Output: Judge-specific types and scoring algorithms
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-judge-commentary/05-CONTEXT.md
@.planning/phases/05-judge-commentary/05-RESEARCH.md
@src/types/debate.ts
@src/store/debateStore.ts
</context>

<tasks>

<task type="auto">
  <name>Create Judge Types Definition</name>
  <files>src/types/judge.ts</files>
  <action>
Create comprehensive type definitions for the judge commentary system:
- JudgeScore interface with 0-100% scores for 5 criteria (content, rebuttal, POI handling, delivery, teamwork)
- Individual criterion scores with 2-3 sub-criteria as specified in context
- Overall score calculation as average of criteria plus holistic adjustment
- JudgeFeedback interface for narrative commentary with strengths/weaknesses
- JudgeEvaluation interface combining scores and feedback
- SpeakerEvaluation mapping each speaker role to their scores and feedback
- ComparativeAnalysis interface for comparing speakers and determining rankings

Use percentage (0-100) scale for all numerical scores as specified in context.
Structure should support the side-by-side comparison layout mentioned in context.
Include specific examples field in feedback for concrete justification.
</action>
  <verify>typescript --noEmit src/types/judge.ts</verify>
  <done>Valid TypeScript types exist for judge scoring and feedback with proper interfaces and enums</done>
</task>

<task type="auto">
  <name>Implement Core Scoring Logic</name>
  <files>src/lib/scoringSystem.ts</files>
  <action>
Create scoring system with the following functionality:
- Function analyzeDebateTranscript(transcript: DebateTranscriptEntry[]) that returns JudgeEvaluation
- Implement 5 evaluation criteria: content, rebuttal, POI handling, delivery, teamwork
- For each main criterion, implement 2-3 sub-criteria for moderate detail
- Calculate scores on 0-100% scale with equal weighting for criteria
- Generate narrative commentary with specific examples from transcript
- Include comparative analysis ranking all 4 speakers 1st-4th with ties allowed
- Use shorthands for transcript analysis: look for argument strength, rebuttal effectiveness, POI acceptance/usage, speaking clarity, and team coordination
- Function should return comprehensive evaluation with both scores and narrative feedback
- Include tie-breaking logic with 2% threshold as recommended in research
</action>
  <verify>Create a mock transcript and verify scoring function returns expected structure</verify>
  <done>Scoring system can analyze transcript and produce comprehensive judge evaluation with scores and feedback</done>
</task>

<task type="auto">
  <name>Add Scoring Utilities</name>
  <files>src/lib/scoringSystem.ts</files>
  <action>
Add utility functions to complement the scoring system:
- Function to calculate overall winner from speaker evaluations
- Helper to format scores for UI display (percentages, rankings)
- Function to generate comparison statements between speakers
- Helper to extract specific quotes/examples from transcript for feedback justification
- Include error handling for edge cases in scoring
- Add logging for debugging scoring decisions
</action>
  <verify>Run sample data through utility functions and verify correct formatting</verify>
  <done>Utility functions support the core scoring system with formatting and comparison capabilities</done>
</task>

</tasks>

<verification>
Verify all types compile correctly and scoring logic produces expected outputs for sample data.
</verification>

<success_criteria>
- Judge types defined with proper TypeScript interfaces
- Scoring system analyzes transcript and produces comprehensive evaluations
- All 5 criteria evaluated with sub-criteria as specified
- Narrative feedback includes specific examples from debate
- Comparative analysis ranks all 4 speakers with tie handling
</success_criteria>

<output>
After completion, create `.planning/phases/05-judge-commentary/05-01-SUMMARY.md`
</output>